{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LLMs and Chat Models are two types of models in LangChain, serving different purpose in NLP tasks.\n",
        "\n",
        "This notebook will examine the differences between LLMs and Chat Models, their unique use cases and how they are implemented within LangChain."
      ],
      "metadata": {
        "id": "JElyRthOyB7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMs such as GPT-3, Bloom, PaLM, and Aurora genAI, take a string as input and return a text string as output.\n",
        "\n",
        "- They are trained on `language modelung tasks` and can generate human-like text, perform complex reasoning, and even write codes.\n",
        "\n",
        "- They are powerful and capable of generating text for wide range of tasks. However, they can sometimes produce incorrrect or nonsensical answers, and their `API is less structures compare to Chat Models.`"
      ],
      "metadata": {
        "id": "Gx4_1i-HyXKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM"
      ],
      "metadata": {
        "id": "vi0WSQa92Efx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use LLM like GPT-3 in LangChain:\n",
        "1. Import `OpenAI` and initialize the desired model\n",
        "2. Create `PromptTemplate` to format the input for the model.\n",
        "3. Define `LLMChain` to combine the model and prompt.\n",
        "4. .run()"
      ],
      "metadata": {
        "id": "02cyvMCjzz3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai langchain_community python-dotenv --quiet"
      ],
      "metadata": {
        "id": "ZI1o9ahD0W6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv('/content/drive/MyDrive/Active Loop RAG/api.env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZoikx7r1LEx",
        "outputId": "91179608-b403-4730-a30c-3e04a9501747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IPJYlqWxtH3"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "JuzlKS4N0e6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes{product}?\"\n",
        ")"
      ],
      "metadata": {
        "id": "1nIjoTsE1CCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "XeDcMGuW1bb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke(\"wireless headphones\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiGkqPox1fj5",
        "outputId": "580c34a7-8a79-4ad7-ba95-1c3c8f8994d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"SoundWave Wireless\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Model"
      ],
      "metadata": {
        "id": "VYso2qpY2DgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat Models take list of messages as input and return an `AIMessage`.\n",
        "\n",
        "These models use LLM as their unerlying technology but their APIs are more structure than LLMs.\n",
        "\n",
        "They are designed to remember previous exchanges with their user in a session and use that context to generate more relebant responses.\n",
        "\n",
        "They learn from reinforcement learning from human feedback which helps improve their responses."
      ],
      "metadata": {
        "id": "KiBzVgDq2LB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_community.callbacks import get_openai_callback\n",
        "from langchain.schema import (\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")"
      ],
      "metadata": {
        "id": "EhcLonVV1qdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(model='gpt-3.5-turbo-0125', temperature=0)"
      ],
      "metadata": {
        "id": "wI4_TvZu3KtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to French language.\"),\n",
        "    HumanMessage(content=\"Translate following sentence: I love programming.\")\n",
        "]"
      ],
      "metadata": {
        "id": "5YfuHdpt3Nm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with get_openai_callback() as cb:\n",
        "  print(chat(messages))\n",
        "print(cb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ORk2WBT3h6M",
        "outputId": "9a5a6b5f-ade6-41bd-f8b8-1be7d97864fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"J'adore la programmation.\" response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 31, 'total_tokens': 39}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-cada8044-bddf-4961-a4ce-7f3325a3cc73-0'\n",
            "Tokens Used: 39\n",
            "\tPrompt Tokens: 31\n",
            "\tCompletion Tokens: 8\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $2.75e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We send the list of messages to the chat model using chat() function and the model returns the result.\n",
        "\n",
        "`SystemMessage` represents the messages generated by the system that wants to use the model, which could include instructions, notifications or error messages.\n",
        "\n",
        "These messages are not generated by the humans of AI chatbot but are instead produced by the underlying system to provide context, guidance or status updates."
      ],
      "metadata": {
        "id": "YdJ-DGiw8vOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_messages = [\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to French language.\"),\n",
        "        HumanMessage(content=\"Translate following sentence: I love programming.\")\n",
        "    ],\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates French to English language.\"),\n",
        "        HumanMessage(content=\"Translate following sentence: J'aime la programmation.\")\n",
        "    ]\n",
        "]"
      ],
      "metadata": {
        "id": "jQm7pHWm3r7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with get_openai_callback() as cb:\n",
        "  print(chat.generate(batch_messages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7fFrieq9hU6",
        "outputId": "51afda1c-c368-48ee-df0c-87089599bce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generations=[[ChatGeneration(text=\"J'adore la programmation.\", generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content=\"J'adore la programmation.\", response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 31, 'total_tokens': 39}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-50555220-df9d-4c81-b193-29cb1237194e-0'))], [ChatGeneration(text='I like programming.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='I like programming.', response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 34, 'total_tokens': 38}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d44c1dd1-6eb7-4caf-b8c9-8d4b44fdca19-0'))]] llm_output={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 65, 'total_tokens': 77}, 'model_name': 'gpt-3.5-turbo-0125'} run=[RunInfo(run_id=UUID('50555220-df9d-4c81-b193-29cb1237194e')), RunInfo(run_id=UUID('d44c1dd1-6eb7-4caf-b8c9-8d4b44fdca19'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeaGikQM9r-c",
        "outputId": "ef214071-703f-4abd-9285-cbf61768452d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens Used: 77\n",
            "\tPrompt Tokens: 65\n",
            "\tCompletion Tokens: 12\n",
            "Successful Requests: 2\n",
            "Total Cost (USD): $5.050000000000001e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "5.050000000000001e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP0xU9dq9ufp",
        "outputId": "a9685c25-ca4b-4eb6-a8bd-b9fb361f72e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.050000000000001e-06"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}