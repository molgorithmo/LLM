{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Conversational capabilities with ChatGPT"
      ],
      "metadata": {
        "id": "trx7l3Vc0160"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I will try to explain how to maintain the context in conversations. Hoe these models remember conversation history and respond accordingly.\n",
        "\n",
        "Also, will breifly discuss the improvement in GPT-4 such as **longer context length** and **better geberalization.**\n",
        "\n",
        "Benefits of employing ChatGPT and GPT-4 chat format:\n",
        "1. GPT-4's short term memory capacity of 64,000 words which surpass the GPT-3.5's 8,000 word limit , enabling it to maintain context more efficiently.\n",
        "2. GPT-4 is bilingual, accurately handling upto 26 languages.\n",
        "3. New model is boasted to have a 40% in factual responses and an 82% reduction in disallowed content reponses.\n",
        "4. Multi-modal"
      ],
      "metadata": {
        "id": "hymf61FO0_ew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9FITwv10qMZ",
        "outputId": "0792676c-a97f-48ad-83c3-79261800b938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m690.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain-openai langchain_community python-dotenv --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv('/content/drive/MyDrive/Active Loop RAG/api.env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd9e49q50wNH",
        "outputId": "8aa426af-c98f-4454-b491-3e7f882d9f57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.schema import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    AIMessage\n",
        ")"
      ],
      "metadata": {
        "id": "1aiA2PAs2iLn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages=[\n",
        "    SystemMessage(content = 'You are a helpful assistant.'),\n",
        "    HumanMessage(content = 'What is the capital of Paris?'),\n",
        "    AIMessage(content = 'The capital of paris is France.')\n",
        "]"
      ],
      "metadata": {
        "id": "KTE6OqlH20L3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(content=\"What community has the majority in the city that you mentioned?\")"
      ],
      "metadata": {
        "id": "JtRRuM423D5m"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(prompt)"
      ],
      "metadata": {
        "id": "V9HoMG7N3auG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name='gpt-4')"
      ],
      "metadata": {
        "id": "YFE81tS83dZm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with get_openai_callback() as cb:\n",
        "  print(llm(messages))\n",
        "print(cb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKuqkURF3iUG",
        "outputId": "916cd3f9-9d21-4c53-ebb2-854cc2fd95a4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-c0bc0509cceb>:2: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
            "  print(llm(messages))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The majority community in Paris, France is the French.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 51, 'total_tokens': 62}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-56820742-f4ef-4174-88fc-5aeb1a177c06-0' usage_metadata={'input_tokens': 51, 'output_tokens': 11, 'total_tokens': 62}\n",
            "Tokens Used: 62\n",
            "\tPrompt Tokens: 51\n",
            "\tCompletion Tokens: 11\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.0021899999999999997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6o6riiKx4EV2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}